== User Guide
Streamliner uses configuration files which applied to templates to create DDL and DML statements which produce complex data ingestion pipelines.  There are two main function performed in Streamliner:
1. `schema` metadata parsing - Collects table metadata information from a relational database or Amazon Web Services Glue Data Catalog.  The output of the `schema` command enhances the original configuration to the be applied to pipeline templates.
2. `scripts` DDL and DML creation - Applies configuration properties to templates to produce DDL and DML scripts.  Templates are a collection of operations producing a data ingestion pipeline for a single table.

== Compatible JDK
Streamliner 5.x compatible JDK is Java 11.

== Configuration
The initial Streamliner configuration contains properties about the data source (Jdbc or Glue AWS data catalog) and destinations (Hadoop or Snowflake).  The configurations are created as yaml files and passed into the `schema` and `scripts` subcommands on execution.

Environment variable can be set to the property. Example `url: "${env:URL}"`. Streamliner will read the url from the environment variable URL.

=== Pipeline Configuration
The base level pipeline configuration has 3 properties that define the pipeline.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| name | String | True | A unique identifier for the pipeline (ex: JD Edwards North America)
| environment | String | True | Environmental classifier (ex: DEV, TEST, PROD)
| pipeline | String | True | The name of the pipeline template (ex: snowflake-snowpipe-append or truncate-reload)
| source | Source | True | <<Data Source Configuration, Data Source Configuration>>
| destination | Destination | True | <<Destination Configuration, Destination Configuration>>
| genericProperties | Map[Object, Object] | False | <<Generic Properties, Generic Properties>>
|===

=== Generic Properties
It can be used to pass any generic parameter that are not pre-defined in the model classes. This attribute will be at the same level of source and destination.

For example, we can pass `tableName` using `genericProperties` like below:

[source,yaml]
----
source:
  // attributes
destination:
  // attributes
genericProperties:
   tableName: "employee"
----

It can be accessed in the template with `${configuration.genericProperties.get("tableName")}`

=== Data Source Configuration
Streamliner collects metadata from two types of sources:

- <<Jdbc Data Source, Jdbc>>
- <<AWS Glue Data Catalog, Glue>>

Only one `source` object can be defined per ingest configuration.

==== Jdbc Data Source
The Jdbc data source configuration defines connection strings and other attributes for relational database management systems.  These configurations will be used by SchemaCrawler to collect metadata on the source system and enhance the configuration with table and column level metadata.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| type | String | True | Jdbc
| url | String | True | The jdbc url for the source system (ex: jdbc:oracle:thin:@{host}:{port}:{sid})
| username | String | True | JDBC connection username
| passwordFile | String | True | Only applicable when destination is Hadoop, should be empty string when destination is Snowflake which will use `--database-password`. Location of the password file stored using Hadoop File System.
| schema | String | True | The schema on the source system to parse metadata from.
| tableTypes | List[String] | True | Controls which objects get parsed on the source system acceptable values are **table** and **view**
| metadata | Map[String, String] | False | Global metadata map of key value pairs added as metadata on all tables at creation
| userDefinedTable | List[UserDefinedTable] | False | <<User Defined Table, User Defined Table>> attributes (override tables metadata to be parsed)
| tables | List[String] | False | Controls which tables to be crawled from source system.
| ignoreTables | List[String] | False | Controls which tables  to be ignored from schema crawling.
| validSchemaChanges | List[String] | False | Controls the <<Allowed Schema Changes, Allowed Schema Changes>> in schema evolution.
| includeHiveAttributes | Boolean | False | Controls the <<Hive attribute collection, Hive attribute collection>>. Default value is false.
|===

=== Hive attribute collection
Only applicable when source is Hive/Impala. Hive attributes `InputFormat` and `Location` can be collected using an optional boolean parameter `includeHiveAttributes` under <<Jdbc Data Source, Jdbc Data Source>> in <<Pipeline Configuration, Pipeline Configuration>>.
To enable this feature, user has to include it in the pipeline configuration as `includeHiveAttributes: true`. The attributes are collected in the output state files in the `--state-directory` folder.

This is an example of the attributes collected in the state file.

[source,yaml]
----
// other parameters
fileFormat:
    location: "hdfs://nameservice1/user/hive/data/avro_only_sql_columns"
    fileType: "AVRO"
----

Hive `Location` and `InputFormat` is mapped to <<File Format, FileFormat>> `location` and `fileType` respectively.

User will not get the exact `InputFormat` value, rather a snowflake https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html[file format type] equivalent value.

Below is the Hive `InputFormat` and Snowflake file format mapping.

[%autowidth.stretch]
|===
| **Hive InputFormat** | **Snowflake File Format**
| org.apache.hadoop.mapred.TextInputFormat | CSV
| org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat | AVRO
| org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat | PARQUET
| org.apache.hadoop.hive.ql.io.orc.OrcInputFormat | ORC
|===

==== Allowed Schema Changes
* TABLE_ADD
* COLUMN_ADD
* UPDATE_COLUMN_COMMENT
* UPDATE_COLUMN_NULLABILITY
* EXTEND_COLUMN_LENGTH

==== AWS Glue Data Catalog
The AWS Glue data catalog configuration defines which Glue Data Catalog database to collect metadata from.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| type | String | True | Glue
| region | String | True | AWS region (ex. us-east-1)
| database | String | True | AWS Glue data catalog database to parse table list from
| userDefinedTable | List[UserDefinedTable] | False | <<User Defined Table, User defined table>> attributes
|===

===== User Defined Table
The user defined table configuration object allows the user to enhance the metadata about a specific table with custom properties.

===== Hadoop User Defined Table
Additional metadata properties for Hadoop specific templates.  Allows the user to supply metadata properties that are not discoverable on the source RDMS or Glue systems.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| type | String | True | Hadoop
| name | String | True | Source system table name
| primaryKeys | Seq[String] | False | A user provided list of primary key columns
| checkColumn | String | False | The incremental column that is 'checked when using Sqoop incremental import
| numberOfMappers | String | False | The number of mappers to be used by Sqoop during import
| numberOfPartitions | String | False | The number of partitions to be created when using pipelines that create Kudu tables
| metadata | Map[String, String] | False | Table metadata map of key value pairs added as metadata on all tables at creation
|===

===== Snowflake User Defined Table
Additional metadata properties for Snowflake specific templates.  Allows the user to supply metadata properties that are not discoverable on the source DMS or Glue systems.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| type | String | True | Snowflake
| name | String | True | Source system table name
| primaryKeys | Seq[String] | False | A user provided list of primary key columns
| fileFormat | <<File Format, FileFormat>> | False | User provided file format definition
|===

====== File Format
Snowflake file format definition

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| location | String | True | Cloud storage location
| fileType | String | True | File type
| delimiter | String | False | Field delimiter used on separated text file formats
| nullIf | Seq[String] | False | A list of values that should be converted to NULL if found on ingest
|===

=== Destination Configuration
Properties defining the data platform being ingested into.  Streamline supports the following destination types:

- <<Hadoop Destination, Hadoop>>
- <<Snowflake Destination, Snowflake>>

Only one destination configuration can be defined per ingest configuration.

==== Hadoop Destination
Configuration properties defining the Hadoop destination.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| type | String | True | Hadoop
| impalaShellCommand | String | True | Impala shell command used to execute sql statements from produced DML and DDL scripts
| stagingDatabase | <<Hadoop Database, HadoopDatabase>> | True | Staging database in the Hadoop environment
| reportingDatabase | <<Hadoop Database, HadoopDatabase>>  | True | Reporting or Modeled database in the Hadoop environment
|===

===== Hadoop Database
Configuration properties defining the Hadoop database.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| name | String | True | The name of the Hadoop database
| path | String | True | The HDFS path to the Hadoop database
|===

==== Snowflake Destination
Configuration properties defining the Snowflake destination.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| type | String | True | Snowflake
| snowSqlCommand | String | True | SnowSQL cli command used to execute sql statements from produced DML and DDL scripts
| storagePath | String | True | Cloud storage location to find the schema level table objects from (used when creating external stage)
| storageIntegration | String | True | Name of the Snowflake storage integration used when created an external stage
| snsTopic | String | False | Name of the AWS SNS Topic to be used when configuring Snowpipe.
| warehouse | String | True | Name of the Snowflake warehouse to be used when executing tasks and copy into commands
| taskSchedule | String | False | Snowflake task schedule
| stagingDatabase | <<Snowflake Database, SnowflakeDatabase>> | True | Staging database where data will staged into in Snowflake
| reportingDatabase | <<Snowflake Database, SnowflakeDatabase>> | True | Reporting database where data will be merged into or replicated in Snowflake
| tableNameStrategy | <<Table Name Strategy, TableNameStrategy>> | False | Option to modify snowflake table name.
| pipeErrorIntegration | String | False | Snowpipe notification integration name.
| taskErrorIntegration | String | False | Snowflake Task notification integration name.
|===

===== Snowflake Database
Configuration properties defining the Snowflake database.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| name | String | True | Snowflake database name
| schema | String | True | Snowflake schema name
|===

===== Table Name Strategy
Configuration properties defining the `tableNameStrategy`.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| asIs | Boolean | False | No change in the snowflake tables name.
| addPostfix | String | False | This will add a postfix to the snowflake tables name.
| addPrefix | String | False | This will add a prefix to the snowflake tables name.
| searchReplace | <<Search Replace, SearchReplace>> | False | Option to search and replace string in the snowflake table name.
|===

===== Search Replace
Configuration properties defining the `searchReplace`.

[%autowidth.stretch]
|===
| **Property** | **Data Type** | **Required** | **Comment**
| search | String | True | String to be searched in the table name. Regex can also be used.
| replace | String | True | String to replace the searched string.
| occurrences | Integer[] | False | The searched string will be replaced only at provided occurrence values.
|===

=== Initial Configuration Example

==== Jdbc Hadoop Configuration

[source,yaml]
----
name: JDEdwardsNorthAmerica
environment: prod
pipeline: incremental-with-kudu
source:
  type: Jdbc
  url: "jdbc:oracle:thin:@{host}:{port}:{sid}"
  username: INGEST_USER
  passwordFile: "/home/ingest_user/jdedwards_na/.password"
  schema: "JDE"
  tableTypes:
    - table
destination:
  type: Hadoop
  impalaShellCommand: impala-shell -f
  stagingDatabase:
    name: JDE_NA_RAW
    path: "hdfs:/data/raw/jde_na/"
  reportingDatabase:
    name: JDE_NA
    path: "hdfs:/data/modeled/jde_na"
----

==== Glue Snowflake Configuration

[source,yaml]
----
name: InternalSalesForecastingDatabase
environment: prod
pipeline: snowflake-snowpipe-append
source:
  type: Glue
  region: us-east-1
  database: sales_forecast
destination:
  type: Snowflake
  snowSqlCommand: snowsql -c phdata
  storagePath: "s3://{bucket}/{path}"
  storageIntegration: "STORAGE_INTEGRATION_READ"
  warehouse: "DEFAULT_WH"
  taskSchedule: "5 minutes"
  stagingDatabase:
    name: SALES_FORECAST
    schema: STAGE
  reportingDatabase:
    name: SALES_FORECAST
    schema: RAW
----

== Templates
Templates define a data ingestion pipeline for a single table within a schema or database.  Templates generate DDL and DML statements and are written using https://scalate.github.io/scalate/documentation/ssp-reference.html[Scala Server Pages]. Streamliner has templates for automating data pipelines in Snowflake and Hadoop data platforms.

=== Make targets

Targets are a function of a template. Therefore, it's possible for a template to define custom targets. The table below
documents standards we expect all templates to follow.

==== Command Types

[%autowidth.stretch]
|===
| **Syntax**      | **Description**
|`first-run` | `first-run` will create tables and provision objects necessary to complete the data pipeline. For example creating a Snowflake stage.
|`run`        | `run` is executed on an on-going basis and should be scheduled. These are typically only used for Hadoop based batch data pipelines.
|`evolve-schema` |`evolve-schema` keeps your target destination tables in sync with a source system and should be scheduled. This command stores state in source control. If you have already executed `first-run` then you will need to generate the appropriate state.
|`drop`        |`drop` will drop or destroy all objects related to the data pipeline.
|===

==== Actual Commands

[%autowidth.stretch]
|===
| **Syntax**      | **Description**
|`first-run-all` | Run `first-run` on all configured tables.
|`first-run-<TABLE_NAME>`  | Run `first-run` only on `<TABLE_NAME>`
|`run-all` | Run `run` on all configured tables.
|`run-<TABLE_NAME>`  | Run `run` only on `<TABLE_NAME>`
|`evolve-schema-all` | Run `evolve-schema` on all configured tables.
|`evolve-schema-<TABLE_NAME>`  | Run `evolve-schema` only on `<TABLE_NAME>`
|`drop-all` | Run `drop` on all configured tables.
|`drop-<TABLE_NAME>`  | Run `drop` only on `<TABLE_NAME>`
|===


=== Hadoop Templates
==== Incremental With Kudu
See `templates/hadoop/incremental-with-kudu`
Creates an incremental ingestion pipeline using Sqoop and Apache Kudu.

`first-run` Steps:

1.  Create a sqoop job definition for incremal batch loading into Hadoop avro table
2. Execute sqoop job full load
3. Copy avsc.json (avro schema) to archive table location
4. Copy avsc.json (avro schema) to staging table location
5. Create archive table.  The archive table appends all incremental sqoop runs into a single table for auditing.
6. Create report Kudu table.
7. Create staging table.  The staging table which sources the UPSERT INTO kudu table operation.
8. Compute stats on the staging or increment table.
9. UPSERT INTO kudu table FROM staging table
10. Compute states on the kudu table

`run` Steps:

1. Copy last increment into archive table
2. Invalidate metadata on archive table to refresh newly loaded increment
3. Compute stats on the archive table
4. Invalidate metadata on Staging table to remove reference to previously loaded files
5. Execute sqoop job incremental
6. Copy avsc.json (avro schema) to archive table location
7. Copy avsc.json (avro schema) to staging table location
8. Compute stats on the staging or increment table.
9. UPSERT INTO kudu table FROM staging table
10. Compute states on the kudu table

`drop` Steps:

1. Delete the sqoop job definition
2. Drop the archive table
3. Drop the report table
4. Drop the staging table
5. Delete the archive HDFS data
6. Delete the staging HDFS data
7. Delete the generated avro schema definitions

==== Kudu Table DDL
See `templates/hadoop/kudu-table-ddl`
Creates kudu tables.

`first-run` Steps:

1. Create Kudu table
2. Compute stats on Kudu table

`run` Steps:

1. Compute stats on Kudu table

`drop` Steps:

1. Drop Kudu table

==== Truncate Reload
See `templates/hadoop/truncate-reload`

Build a full truncate and reload data pipeline using Sqoop and Impala tables.  Snapshots of the last 5 data loads are kept in an archive table to reduce the need to re-run the ingest from source if data needs to be reloaded in the Impala table.

`first-run` Steps:

1. Execute Sqoop full load job
2. Copy avsc.json (avro schema) to hdfs table directory
3. Create partitioned or archive table
4. Create reporting table
5. Create staging table
6. Invalidate Impala meta data on staging table
7. Switch the reporting table location to latest loaded partition
8. Compute stats of partitioned table
9. Compute stats on reporting table
10.  Validate row count between source system and Impala table

`run` Steps:

1. Execute Sqoop job
2. Copy avsc.json (avro schema) to hdfs table directory
3. Invalidate Impala metadata on staging table
4. Compute stats on staging table
5. Compute stats on reporting table
6. Validate row count between source system and Impala table

`drop` Steps:

1. Drop partitioned table
2. Drop report table
3. Drop staging table
4. Delete partitioned HDFS data
5. Delete reporting HDFS data
6. Delete staging HDFS data
7. Delete generated HDFS avro schema

=== Snowflake Templates
==== Snowflake AWS DMS Merge
See `templates/snowflake/snowflake-aws-dms-merge`

The AWS DMS Merge template is Snowflake data pipeline incrementally ingesting CDC records from Amazon Database Migration (DMS) service into Snowflake.

Snowflake's continous ingest tool Snowpipe automatically copies data into the staging table once the files are written to the external stage.  A scheduled Snowflake Task then merges the records into the reporting table.

*NOTE:* After `run` is executed once there is no need to schedule this template externally.

`first-run` Steps:

1. Create staging schema
2. Create staging table
3. Create report schema
4. Create report table
5. Create external stage for table referencing Snowflake Storage integration
6. Create Snowflake Stream on staging table for incremental change tracking
7. COPY INTO staging table bulk historical records
8. Create Snowflake Task to MERGE INTO reporting from Stream
9. Schedule task execution

`run` Steps:

1. Copy any incremental CDC events into staging table
2. Create Snowpipe for ongoing ingestion

`drop` Steps:

1. Suspend task execution
2. Drop Snowflake task
3. Drop Snowpipe
4. Drop Stream
5. Drop report table
6. Drop staging table

*NOTE:* `drop` does not remove the `staging` and `report` schemas nor does it the `stage` which can be cleaned by `make drop-stage`.

==== Snowflake Snowpipe Append
See `templates/snowflake/snowflake-snowpipe-append`

The Snowpipe Append template is A Snowflake data pipeline to append newly arriving data from a Storage Integration into Snowflake.  Snowpipe is used to continously load data from cloud storage accounts into Snowflake.

*NOTE:* Once `first-run` is executed this pipeline does not need to be scheduled by externally.

`first-run` Steps:

1. Create schema
2. Create stage
3. Create table
4. Copy into table from external stage
5. Create Snowpipe

`drop` Steps:

1. Drop table
2. Drop Snowpipe

*NOTE:* `drop` does not remove the schema nor does it the `stage` which can be cleaned by `make drop-stage`.

== QA Process

=== Overview
When creating data pipelines, measuring quality is always very important. Streamliner supports templates for collecting metrics
on columns of tables and compare them over time. This method is based off of https://en.wikipedia.org/wiki/Control_chart[control charts]
to measure the variablility of the amount of data over time.

=== Process
To take a control chart process and convert it to pipelines the process needs to complete the following steps:

1.	Capture the occurrences of data values across all columns in a table.
2.	Not capture data that is small or a small percentage of the values in a column.
3.	Store each “run” of data in a metrics vault.
4.	Compare the current “run” to the average and standard deviation of the previous runs for each value in each column of the table.

Starting out with collecting and storing the data, in every database using this process we would want to create a “Metrics” table to store all of the data. That table would have the time and date of the run, the column name being looked at, the value in that column, and the count of records with that value. To collect the data in snowflake, a stored procedure can find all the columns on the table, calculate the counts for each value, and insert the data into the “Metrics” table.

To easily collect the data, we can add a stream to the table we are monitoring and have a task that runs the stored procedure to collect the data. Once the data is collected and stored, the stored procedure can check if the values are within the UCL and LCL or 3 standard deviations away from the historical mean. If there are any values outside the range, the stored procedure can call an external api alerting users that some of the date requires investigation. Below is a diagram showing the dataflow.

image::../images/streamliner-qa-process.png[QA_process]


==== Dos and Don'ts
This process should be used when monitoring on ongoing pipeline. It will detect if a statistical
anomaly in the values of the data. It will not tell you if something is wrong or confirm data fits a 
predefined structure. The metrics repo also needs data to inform if data fits the
historical pattern, so it will take time before the results from the QA Proces become dependable.

Constraints on tables, proper monitoring of logs, and unit tests are still recommended.

The metrics repo can also be connected to from a visualization tool to allow
users dig into the values and how they change over time.

== Installing Streamliner
You can find the latest version of streamliner in https://cloudsmith.io/~phdata/repos/streamliner/packages/[phData's Artifactory].

The artifact is a zip file contains an executable in `bin/streamliner`, templates in `templates`, and example config in `conf`.

== Executing Streamliner

=== Schema Parsing
Before executing the schema parsing functionality of Streamliner the developer must first create the <<Initial Configuration Example, initial ingest configuration>>

CLI Arguments:

[%autowidth.stretch]
|===
| **Name** | **Type** | **Required** | **Comment**
| config | String | True | Location of the initial ingest configuration file
| state-directory | String | True | Current run table config directory where Streamliner configuration per table will be written.
| previous-state-directory | String | True | Previous run table config directory where Streamliner configuration per table is written to.
| database-password | String | False | Relational database password used when parsing Jdbc source types. Not used when importing to Hadoop which uses the `passwordFile` Yaml key.
| create-docs | Boolean | False | Control flag to indicate whether an ERD and HTML file should be created when parsing Jdbc source types.
| log-level | String | False | Parameter to change the application log level. Log level set in `log4j.properties` file present in `conf` folder is the default value.
|===

CMD:

----
<install directory>/bin/streamliner schema --config conf/private-ingest-configuration.yml --state-directory <state-directory-path> --database-password <pass> --log-level INFO
----

=== Script Generation
Schema parsing must be executed before executing script generation as the table definitions are needed to create the data pipelines.

CLI Arguments:

[%autowidth.stretch]
|===
| **Name** | **Type** | **Required** | **Comment**
| config | String | True | Location of the initial ingest configuration file
| state-directory | String | True | Current run table config directory where Streamliner configuration per table is written to.
| previous-state-directory | String | True | Previous run table config directory where Streamliner configuration per table was written to.
| output-path | String | True | Location where Streamliner output should be written to.
| type-mapping | String | True | Location of the type-mapping.yml file
| template-directory | String | True | Location of the templates
| log-level | String | False | Parameter to change the application log level. Log level set in `log4j.properties` file present in `conf` folder is the default value.
|===

CMD : `<install directory>/bin/streamliner scripts --config conf/private-ingest-configuration.yml --state-directory <state-directory-path> --previous-state-directory <previous-state-directory-path> --type-mapping <install-directory>/conf/type-mapping.yml  --template-directory <install directory>/templates/<snowflake | hadoop> --output-path <script-output-path> --log-level INFO`

== Migrating Templates from Streamliner 4.x to 5.x
* Replace the imports and variables to use new Java POJO classes.

Example :

Streamliner 4.x:
----
#import(io.phdata.streamliner.configuration.Snowflake)
<%@ val configuration: io.phdata.streamliner.configuration.Configuration %>
----

Streamliner 5.x:
----
#import(io.phdata.streamliner.schemadefiner.model.Snowflake)
<%@ val configuration: io.phdata.streamliner.schemadefiner.model.Configuration %>
----

* Replace the old scala POJO class methods with equivalent new POJO class method definition. Pass the arguments correctly if needed.

Example :

Streamliner 4.x:
----
CREATE TASK IF NOT EXISTS ${destination.stagingDatabase.name}.${destination.stagingDatabase.schema}.${table.destinationName}_task
WAREHOUSE = ${destination.warehouse}
SCHEDULE = '${destination.taskSchedule.getOrElse("5 minutes")}'
WHEN SYSTEM$STREAM_HAS_DATA('${table.destinationName}_stg_stream')
AS
MERGE INTO ${destination.reportingDatabase.name}.${destination.reportingDatabase.schema}.${table.destinationName} t
    USING ( SELECT ${table.columnList(Some("si"))}, si.dms_operation, i.max_dms_ts
            FROM ${destination.stagingDatabase.name}.${destination.stagingDatabase.schema}.${table.destinationName}_stg_stream si
            INNER JOIN ( SELECT ${table.pkList}, MAX(dms_ts) max_dms_ts
                         FROM ${destination.stagingDatabase.name}.${destination.stagingDatabase.schema}.${table.destinationName}_stg_stream
                         GROUP BY ${table.pkList} ) i
            ON ${table.pkConstraint("i", "si")} AND i.max_dms_ts = si.dms_ts ) s
ON ${table.pkConstraint("t", "s")}
    WHEN MATCHED AND s.dms_operation = 'U' THEN UPDATE SET ${table.columnConstraint(bAlias = "s", joinCondition = ", ")}, dms_operation = s.dms_operation, dms_ts = s.max_dms_ts
    WHEN MATCHED AND s.dms_operation = 'D' THEN DELETE
    WHEN NOT MATCHED AND s.dms_operation != 'D' OR s.dms_operation IS NULL THEN INSERT (${table.columnList()}, dms_operation, dms_ts) VALUES (${table.columnList(Some("s"))}, s.dms_operation, s.max_dms_ts);
----

Streamliner 5.x:
----
CREATE TASK IF NOT EXISTS ${destination.stagingDatabase.name}.${destination.stagingDatabase.schema}.${table.destinationName}_task
WAREHOUSE = ${destination.warehouse}
SCHEDULE = '${destination.taskSchedule}'
WHEN SYSTEM$STREAM_HAS_DATA('${table.destinationName}_stg_stream')
AS
MERGE INTO ${destination.reportingDatabase.name}.${destination.reportingDatabase.schema}.${table.destinationName} t
    USING ( SELECT ${table.columnList("si")}, si.dms_operation, i.max_dms_ts
            FROM ${destination.stagingDatabase.name}.${destination.stagingDatabase.schema}.${table.destinationName}_stg_stream si
            INNER JOIN ( SELECT ${table.pkList}, MAX(dms_ts) max_dms_ts
                         FROM ${destination.stagingDatabase.name}.${destination.stagingDatabase.schema}.${table.destinationName}_stg_stream
                         GROUP BY ${table.pkList} ) i
            ON ${table.pkConstraint("i", "si", null)} AND i.max_dms_ts = si.dms_ts ) s
ON ${table.pkConstraint("t", "s", null)}
    WHEN MATCHED AND s.dms_operation = 'U' THEN UPDATE SET ${table.columnConstraint(null, "s", ", ")}, dms_operation = s.dms_operation, dms_ts = s.max_dms_ts
    WHEN MATCHED AND s.dms_operation = 'D' THEN DELETE
    WHEN NOT MATCHED AND s.dms_operation != 'D' OR s.dms_operation IS NULL THEN INSERT (${table.columnList(null)}, dms_operation, dms_ts) VALUES (${table.columnList("s")}, s.dms_operation, s.max_dms_ts);
----

* Since Streamliner 5.x is written in JAVA, we might have to convert few Java code into Scala to support in SSP template. For example, we converted Java List to Scala Seq to use few scala methods in SSP template.

Example :

Streamliner 4.x:
----
CREATE TABLE IF NOT EXISTS ${destination.reportingDatabase.name}.${destination.reportingDatabase.schema}.${table.destinationName} (
#for (column <- table.columns)
${unescape(column.destinationName)} ${column.mapDataTypeSnowflake(typeMapping)} COMMENT '${column.comment.getOrElse("")}',
#end
dms_operation CHAR COMMENT 'AWS DMS Operation type',
dms_ts TIMESTAMP_LTZ COMMENT 'AWS DMS timestamp'
)
COMMENT = '${table.comment.getOrElse("")}';
----

Streamliner 5.x:
----
CREATE TABLE IF NOT EXISTS ${destination.reportingDatabase.name}.${destination.reportingDatabase.schema}.${table.destinationName} (
#for (column <- util.convertListToSeq(table.columns))
${unescape(column.destinationName)} ${column.mapDataTypeSnowflake(typeMapping)} COMMENT '${column.comment}',
#end
dms_operation CHAR COMMENT 'AWS DMS Operation type',
dms_ts TIMESTAMP_LTZ COMMENT 'AWS DMS timestamp'
)
COMMENT = '${table.comment}';
----

== Connect streamliner to hive with jdbc kerberos authentication

1. Connect to hive edge node.
2. Create ticket cache using `kinit` command. Or to create ticket cache using keytab file, please follow these steps:
   * Execute `ktutil` command.
   * Execute
    ```
       add_entry -password -p yourusername@YOURDOMAIN -k 1 -e aes256-cts
    ```
   * Provide password for yourusername@YOURDOMAIN
   * Execute
    ```
        wkt yourusername.keytab
    ```
   * Execute `exit` command.
   * Till step 5, a keytab should be created.
   * To create the ticket cache using keytab file, execute command
    ```
        kinit yourusername@YOURDOMAIN -k -t yourusername.keytab
    ```
3. Validate hive jdbc connection using beeline. Please make sure ticket cache exists and it's not expired using `klist` command.
   ```
       beeline -u "jdbc:hive2://<host_name>:<port>/<db>;principal=<hive_princ_name>"
   ```
4. Copy streamliner to hive edge node.
5. Provide correct hive jdbc url to streamliner config. The hive jdbc url should look like below
    ```
        jdbc:hive2://<host_name>:<port>/<db>;AuthMech=1;KrbHostFQDN=<host_name>;KrbServiceName=hive;KrbRealm=<Realm>;SSL=1;SSLTrustStore=<ssl_trust_store>
    ```
6. Execute streamliner schema command from streamliner folder. Please make sure ticket cache exist before executing schema command.

== Connect streamliner to impala with jdbc kerberos authentication

Steps are same as <<Connect streamliner to hive with jdbc kerberos authentication, Connect streamliner to hive with jdbc kerberos authentication>> except the impala jdbc url pattern.

Provide below impala jdbc url to streamliner config.

[source]
----
    "jdbc:impala://<host_name>:<port>/<db>;AuthMech=1;KrbHostFQDN=<host_name>;KrbServiceName=impala;KrbRealm=<Realm>;SSLTrustStore=<ssl_trust_store>"
----
