<%@ val configuration: io.phdata.pipewrench.configuration.Configuration %>
<%@ val table: io.phdata.pipewrench.configuration.TableDefinition %>
<% val tableColumns = io.phdata.pipewrench.util.TemplateFunction.sourceColumns(configuration, table)%>
<% val javaColumnMap = io.phdata.pipewrench.util.TemplateFunction.sqoopMapJavaColumn(table)%>
#!/usr/bin/env bash

set -euox pipefail

sqoop job -D 'mapreduce.job.name=Sqoop Incremental Job - name: ${configuration.name} environment: ${configuration.environment} table: ${table.destinationName}' \\
    --create '${configuration.name}/${configuration.environment}/${table.destinationName}' \\
    -- import \\
    --driver '${configuration.jdbc.driverClass.get}' \\
    --connect '${configuration.jdbc.url}' \\
    --username '${configuration.jdbc.username}' \\
    --password-file '${configuration.jdbc.passwordFile}' \\
    --incremental append  \\
    --target-dir '${configuration.hadoop.stagingDatabase.path}/stg_${table.destinationName}/' \\
    --temporary-rootdir '${configuration.hadoop.stagingDatabase.path}/stg_${table.destinationName}/' \\
    --append \\
#if (table.splitByColumn.isDefined)
    --splitBy ${table.splitByColumn.get} \\
#elseif (javaColumnMap.isDefined)
    --map-column-java ${javaColumnMap} \\
#end
    --check-column ${table.checkColumn.get} \\
    --as-avrodatafile \\
    --fetch-size 10000 \\
    --compress  \\
    --compression-codec snappy \\
    -m ${table.numberOfMappers.getOrElse(1)} \\
    --query 'SELECT
${tableColumns}
FROM ${configuration.jdbc.schema}.${table.sourceName}
WHERE $CONDITIONS'